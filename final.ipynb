{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uXN-rmcBnnct","outputId":"80d88ae8-3557-42fe-d9d3-86041bcd3872"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import regex as re\n","from bs4 import BeautifulSoup\n","import contractions\n","import unidecode\n","import re\n","import pickle\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","import torch\n","import string\n","from sklearn.utils import shuffle\n","from transformers import AutoTokenizer, AutoModelForMaskedLM,AutoModelForSequenceClassification\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from transformers import TrainingArguments, Trainer\n","from transformers import BertTokenizer, BertForSequenceClassification,RobertaForSequenceClassification,RobertaTokenizer\n","from transformers import EarlyStoppingCallback\n","# from gensim.parsing.preprocessing import remove_stopwords\n","from transformers import XLMRobertaModel, XLMRobertaTokenizer\n","import os\n","# from cleantext import clean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qV7mLFFbnncw"},"outputs":[],"source":["\n","def removeHtml(sentence):\n","    soup = BeautifulSoup(sentence, \"html.parser\")\n","    stripped_text = soup.get_text(separator=\" \")\n","    return stripped_text\n","\n","def removePuncu(sentence):\n","    res = re.sub(r'[^\\w\\s]', '', sentence)\n","    return res\n","\n","def removeLink(sentence):\n","    result = re.sub(r'http\\S+', '', sentence)\n","    return result\n","\n","def removeWhiteSpaces(sentence):\n","    sentence = sentence.strip()\n","    return \" \".join(sentence.split())\n","\n","def expand_contractions(sentence):\n","    sentence = contractions.fix(sentence)\n","    return sentence\n","\n","def removeAccented(sentence):\n","    sentence = unidecode.unidecode(sentence)\n","    return sentence\n","\n","def removeSpec(sentence):\n","    sentence = re.sub(\"[^A-Z]\", \" \", sentence, 0, re.IGNORECASE)\n","    return sentence\n","\n","def removeNum(sentence):\n","    pattern = r'[0-9]'\n","    sentence = re.sub(pattern, '', sentence)\n","    return sentence\n","\n","def lowerCase(sentence):\n","    sentence = sentence.lower()\n","    return sentence\n","\n","# def removeUser(sentence):\n","#     remove = re.compile('(\\s*)user(\\s*)')\n","#     sentence = remove.sub(\" \", sentence)\n","#     return sentence\n","\n","# with open(r'C:\\Users\\LEGION\\Documents\\Untitled Folder 2\\Emoji_Dict.p', 'rb') as fp:\n","#     Emoji_Dict = pickle.load(fp)\n","# Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n","\n","# def convertEmoji(sentence):\n","#     for emot in Emoji_Dict:\n","#         sentence = re.sub(r'('+emot+')', \"\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()),sentence)\n","#     return sentence\n","\n","def add_labels(value):\n","    if value == 'positive':\n","        return 2\n","    elif value == 'negative':\n","        return 0\n","    elif value == 'neutral':\n","        return 1\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZitNor6nncx"},"outputs":[],"source":["def preprocessing(data):\n","    sent_or_word = 'translation_sentence_level'\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: lowerCase(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removePuncu(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeSpec(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeHtml(x))\n","\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeAccented(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeLink(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeNum(x))\n","\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: expand_contractions(x))\n","\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeWhiteSpaces(x))\n","    # data[sent_or_word] = data[sent_or_word].apply(lambda x: removeEmoji(x))\n","    # data[sent_or_word] = data[sent_or_word].apply(lambda x: removeUser(x))\n","\n","\n","    x = data[sent_or_word].to_list()\n","    y = list(data['categorical_labels'])\n","\n","    return x,y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZ7WLIzBnncy"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52jGOTdtnncy"},"outputs":[],"source":["model_results_dict = {\"Model_Name\":[], \"Accuracy\": [], 'Recall': [], 'Precision': [], \"f1\":[]}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCL7hd2Inncy"},"outputs":[],"source":["model_results_dict = {\"Model_Name\":[], \"Accuracy\": [], 'Recall': [], 'Precision': [], \"f1\":[]}\n","\n","max_length = 128\n","num_labels = 3\n","model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_labels, ignore_mismatched_sizes = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYxZ88gsnncy"},"outputs":[],"source":["allTrain = os.listdir('emojiRemovedFiles/train')\n","for file in allTrain:\n","    newModelNameForSaving = model_name.split(\"/\")[-1]\n","\n","    train_data = pd.read_csv('emojiRemovedFiles/train/{}'.format(file), sep = '\\t')\n","    val_data = pd.read_csv('emojiRemovedFiles/val/{}'.format(file) ,sep = '\\t')\n","    train_data = train_data[train_data['translation_sentence_level'].notna()]\n","    val_data = val_data[val_data['translation_sentence_level'].notna()]\n","    train_data['categorical_labels'] = train_data['label'].map(add_labels)\n","    val_data['categorical_labels'] = val_data['label'].map(add_labels)\n","\n","\n","    x_train, y_train = preprocessing(train_data)\n","    x_val, y_val = preprocessing(val_data)\n","\n","    x_train_tokenized = tokenizer(x_train, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n","    x_val_tokenized = tokenizer(x_val, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n","\n","    train_dataset = Dataset(x_train_tokenized, y_train)\n","    val_dataset = Dataset(x_val_tokenized, y_val)\n","\n","\n","    def compute_metrics(p):\n","        pred, labels = p\n","        pred = np.argmax(pred, axis=1)\n","        accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","        recall = recall_score(y_true=labels, y_pred=pred,average='weighted')\n","        precision = precision_score(y_true=labels, y_pred=pred,average='weighted')\n","        f1 = f1_score(y_true=labels, y_pred=pred,average='weighted')\n","\n","        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","    args = TrainingArguments(\n","        output_dir = \"checkpointsForRemoveEmojiTwitterRoberta/{}_{}\".format(newModelNameForSaving, file.split(\".\")[0]),\n","        evaluation_strategy=\"epoch\",\n","        per_device_train_batch_size=256,\n","        per_device_eval_batch_size=256,\n","        num_train_epochs=5,\n","        seed=7,\n","        load_best_model_at_end=True,\n","        save_strategy='epoch',\n","        overwrite_output_dir=True)\n","        #save_strategy = \"no\"\n","\n","    trainer = Trainer(\n","      model=model,\n","      args=args,\n","      train_dataset=train_dataset,\n","      eval_dataset = val_dataset,\n","      compute_metrics = compute_metrics,\n","      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)])\n","\n","\n","    trainer.train()\n","    trainer.save_model(\"modelForRemoveEmojiTwitterRoberta/{}_{}\".format(newModelNameForSaving, file.split(\".\")[0]))\n","\n","    test_data = pd.read_csv(\"emojiRemovedFiles/test/{}\".format(file),sep='\\t')\n","    test_data = test_data[test_data['translation_sentence_level'].notna()]\n","\n","    test_data['categorical_labels'] = test_data['label'].map(add_labels)\n","    print(test_data['translation_sentence_level'].isnull().sum())\n","    X_test, y_test = preprocessing(test_data)\n","\n","\n","    X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n","\n","\n","    test_dataset = Dataset(X_test_tokenized, y_test)\n","\n","\n","    predictions = trainer.predict(test_dataset)\n","\n","    pred = np.argmax(predictions.predictions, axis=-1)\n","\n","\n","    accuracy = accuracy_score(y_true=y_test, y_pred=pred)\n","    recall = recall_score(y_true=y_test, y_pred=pred,average='weighted')\n","    precision = precision_score(y_true=y_test, y_pred=pred,average='weighted')\n","    f1 = f1_score(y_true=y_test, y_pred=pred,average='weighted')\n","\n","\n","    model_results_dict['Model_Name'].append(file)\n","    model_results_dict['Accuracy'].append(accuracy)\n","    model_results_dict['Recall'].append(recall)\n","    model_results_dict['Precision'].append(precision)\n","    model_results_dict['f1'].append(f1)\n","\n","    df = pd.DataFrame(data = model_results_dict)\n","    df.to_csv(\"{}_TwitterRobertaRemoveEmojiresults.csv\".format(newModelNameForSaving))\n"]},{"cell_type":"markdown","metadata":{"id":"MFD8QeW6nncz"},"source":["model_results_dict"]},{"cell_type":"code","source":["def preprocessing(data):\n","    sent_or_word = 'translation_sentence_level'\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: lowerCase(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removePuncu(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeSpec(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeHtml(x))\n","\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeAccented(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeLink(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeNum(x))\n","\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: expand_contractions(x))\n","\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeWhiteSpaces(x))\n","\n","    x = data[sent_or_word].to_list()\n","    y = list(data['categorical_labels'])\n","    return x,y\n","\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])\n","# ------------------------------------------------------------------------------------------------------------------------------------------------\n","model_results_dict = {\"Model_Name\":[], \"Accuracy\": [], 'Recall': [], 'Precision': [], \"f1\":[]}\n","\n","max_length = 128\n","num_labels = 3\n","model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_labels, ignore_mismatched_sizes = True)\n","# ------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","allTrain = os.listdir('emojiRemovedFiles/train')\n","for file in allTrain:\n","    newModelNameForSaving = model_name.split(\"/\")[-1]\n","\n","    train_data = pd.read_csv('emojiRemovedFiles/train/{}'.format(file), sep = '\\t')\n","    val_data = pd.read_csv('emojiRemovedFiles/val/{}'.format(file) ,sep = '\\t')\n","\n","    train_data = train_data[train_data['translation_sentence_level'].notna()]\n","    val_data = val_data[val_data['translation_sentence_level'].notna()]\n","\n","    train_data['categorical_labels'] = train_data['label'].map(add_labels)\n","    val_data['categorical_labels'] = val_data['label'].map(add_labels)\n","\n","\n","    x_train, y_train = preprocessing(train_data)\n","    x_val, y_val = preprocessing(val_data)\n","\n","    x_train_tokenized = tokenizer(x_train, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n","    x_val_tokenized = tokenizer(x_val, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n","\n","    train_dataset = Dataset(x_train_tokenized, y_train)\n","    val_dataset = Dataset(x_val_tokenized, y_val)\n","\n","\n","    def compute_metrics(p):\n","        pred, labels = p\n","        pred = np.argmax(pred, axis=1)\n","        accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","        recall = recall_score(y_true=labels, y_pred=pred,average='weighted')\n","        precision = precision_score(y_true=labels, y_pred=pred,average='weighted')\n","        f1 = f1_score(y_true=labels, y_pred=pred,average='weighted')\n","\n","        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n","\n","    args = TrainingArguments(\n","        output_dir = \"checkpointsForRemoveEmojiTwitterRoberta/{}_{}\".format(newModelNameForSaving, file.split(\".\")[0]),\n","        evaluation_strategy=\"epoch\",\n","        per_device_train_batch_size=256,\n","        per_device_eval_batch_size=256,\n","        num_train_epochs=5,\n","        seed=7,\n","        load_best_model_at_end=True,\n","        save_strategy='epoch',\n","        overwrite_output_dir=True)\n","        #save_strategy = \"no\"\n","\n","    trainer = Trainer(\n","      model=model,\n","      args=args,\n","      train_dataset=train_dataset,\n","      eval_dataset = val_dataset,\n","      compute_metrics = compute_metrics,\n","      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)])\n","\n","\n","    trainer.train()\n","    trainer.save_model(\"modelForRemoveEmojiTwitterRoberta/{}_{}\".format(newModelNameForSaving, file.split(\".\")[0]))\n","\n","    test_data = pd.read_csv(\"emojiRemovedFiles/test/{}\".format(file),sep='\\t')\n","    test_data = test_data[test_data['translation_sentence_level'].notna()]\n","\n","    test_data['categorical_labels'] = test_data['label'].map(add_labels)\n","    print(test_data['translation_sentence_level'].isnull().sum())\n","    X_test, y_test = preprocessing(test_data)\n","\n","    X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n","    test_dataset = Dataset(X_test_tokenized, y_test)\n","    predictions = trainer.predict(test_dataset)\n","    pred = np.argmax(predictions.predictions, axis=-1)\n","\n","    accuracy = accuracy_score(y_true=y_test, y_pred=pred)\n","    recall = recall_score(y_true=y_test, y_pred=pred,average='weighted')\n","    precision = precision_score(y_true=y_test, y_pred=pred,average='weighted')\n","    f1 = f1_score(y_true=y_test, y_pred=pred,average='weighted')\n","\n","    model_results_dict['Model_Name'].append(file)\n","    model_results_dict['Accuracy'].append(accuracy)\n","    model_results_dict['Recall'].append(recall)\n","    model_results_dict['Precision'].append(precision)\n","    model_results_dict['f1'].append(f1)\n","\n","    df = pd.DataFrame(data = model_results_dict)\n","    df.to_csv(\"{}_TwitterRobertaRemoveEmojiresults.csv\".format(newModelNameForSaving))"],"metadata":{"id":"xhiyIHQFu5kG"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.6.5 64-bit ('3.6.5')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"9c5472c625688a4fda46769efa78306e9889cc6db92cebd22ca1efa0057f033e"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}