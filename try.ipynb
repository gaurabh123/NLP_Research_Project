{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import unidecode\n",
    "import re\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "import string\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM,AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,RobertaForSequenceClassification,RobertaTokenizer\n",
    "from transformers import EarlyStoppingCallback\n",
    "# from gensim.parsing.preprocessing import remove_stopwords\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp36-cp36m-linux_x86_64.whl (2041.3 MB)\n",
      "     |████████████████████████████████| 2041.3 MB 17 kB/s              \n",
      "\u001b[?25hCollecting torchvision==0.10.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.1%2Bcu111-cp36-cp36m-linux_x86_64.whl (20.6 MB)\n",
      "     |████████████████████████████████| 20.6 MB 12.1 MB/s            \n",
      "\u001b[?25hCollecting torchaudio==0.9.1\n",
      "  Downloading torchaudio-0.9.1-cp36-cp36m-manylinux1_x86_64.whl (1.9 MB)\n",
      "     |████████████████████████████████| 1.9 MB 10.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages (from torch==1.9.1+cu111) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages (from torch==1.9.1+cu111) (0.8)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages (from torchvision==0.10.1+cu111) (8.4.0)\n",
      "Requirement already satisfied: numpy in /home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages (from torchvision==0.10.1+cu111) (1.19.5)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.2\n",
      "    Uninstalling torch-1.10.2:\n",
      "      Successfully uninstalled torch-1.10.2\n",
      "Successfully installed torch-1.9.1+cu111 torchaudio-0.9.1 torchvision-0.10.1+cu111\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeHtml(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "def removePuncu(sentence):\n",
    "    res = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return res\n",
    "\n",
    "def removeLink(sentence):\n",
    "    result = re.sub(r'http\\S+', '', sentence)\n",
    "    return result\n",
    "\n",
    "def removeWhiteSpaces(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    return \" \".join(sentence.split())\n",
    "\n",
    "def expand_contractions(sentence):\n",
    "    sentence = contractions.fix(sentence)\n",
    "    return sentence\n",
    "\n",
    "def removeAccented(sentence):\n",
    "    sentence = unidecode.unidecode(sentence)\n",
    "    return sentence\n",
    "\n",
    "def removeSpec(sentence):\n",
    "    sentence = re.sub(\"[^A-Z]\", \" \", sentence, 0, re.IGNORECASE)\n",
    "    return sentence\n",
    "\n",
    "def removeNum(sentence):\n",
    "    pattern = r'[0-9]'\n",
    "    sentence = re.sub(pattern, '', sentence)\n",
    "    return sentence\n",
    "\n",
    "def lowerCase(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "# with open(r'C:\\Users\\LEGION\\Documents\\Untitled Folder 2\\Emoji_Dict.p', 'rb') as fp:\n",
    "#     Emoji_Dict = pickle.load(fp)\n",
    "# Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "# def convertEmoji(sentence):\n",
    "#     for emot in Emoji_Dict:\n",
    "#         sentence = re.sub(r'('+emot+')', \"\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()),sentence)\n",
    "#     return sentence\n",
    "\n",
    "def add_labels(value):\n",
    "    if value == 'positive':\n",
    "        return 2\n",
    "    elif value == 'negative':\n",
    "        return 0\n",
    "    elif value == 'neutral':\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred,average='weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred,average='weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred,average='weighted')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "num_labels = 3\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "\n",
    "# safe_model_name = sanitize(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels = num_labels, ignore_mismatched_sizes = True).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_dict = {\"Model_Name\":[], \"accuracy\": [], 'recall': [], 'precision': [], \"f1\":[]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/home/ablstation2/Documents/gaurav/translated_dataset/test/ts_translated_test.tsv', sep = '\\t')\n",
    "val_data = pd.read_csv('/home/ablstation2/Documents/gaurav/translated_dataset/val/ts_translated.tsv', sep = '\\t')\n",
    "train_data['categorical_labels'] = train_data['label'].map(add_labels)\n",
    "val_data['categorical_labels'] = val_data['label'].map(add_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    sent_or_word = 'translation_sentence_level'\n",
    "    data[sent_or_word] = data[sent_or_word].apply(lambda x: lowerCase(x))\n",
    "    data[sent_or_word] = data[sent_or_word].apply(lambda x: removePuncu(x))\n",
    "    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeSpec(x))\n",
    "    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeHtml(x))\n",
    "\n",
    "    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeAccented(x))\n",
    "    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeLink(x))\n",
    "    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeNum(x))\n",
    "\n",
    "    data[sent_or_word] = data[sent_or_word].apply(lambda x: expand_contractions(x))\n",
    "#     da[sent_or_word] = da[sent_or_word].apply(lambda x: convertEmoji(x))\n",
    "    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeWhiteSpaces(x))\n",
    "    x = data[sent_or_word].to_list()\n",
    "    y = list(data['categorical_labels']) \n",
    "    \n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = preprocessing(train_data)\n",
    "x_val, y_val = preprocessing(val_data)\n",
    "\n",
    "x_train_tokenized = tokenizer(x_train, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n",
    "x_val_tokenized = tokenizer(x_val, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n",
    "\n",
    "train_dataset = Dataset(x_train_tokenized, y_train)\n",
    "val_dataset = Dataset(x_val_tokenized, y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'ts_translated.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"/home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/{}_{}\".format(model_name,x.split(\".\")[0]),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    num_train_epochs=5,\n",
    "    seed=7,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy='epoch',\n",
    "    overwrite_output_dir=True\n",
    "      #save_strategy = \"no\"\n",
    "  )\n",
    "\n",
    "trainer = Trainer(\n",
    "      model=model,\n",
    "      args=args,\n",
    "      train_dataset=train_dataset,\n",
    "      eval_dataset=val_dataset,\n",
    "      compute_metrics=compute_metrics,\n",
    "      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeHtml(sentence):\n",
    "    soup = BeautifulSoup(sentence, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "def removePuncu(sentence):\n",
    "    res = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return res\n",
    "\n",
    "def removeLink(sentence):\n",
    "    result = re.sub(r'http\\S+', '', sentence)\n",
    "    return result\n",
    "\n",
    "def removeWhiteSpaces(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    return \" \".join(sentence.split())\n",
    "\n",
    "def expand_contractions(sentence):\n",
    "    sentence = contractions.fix(sentence)\n",
    "    return sentence\n",
    "\n",
    "def removeAccented(sentence):\n",
    "    sentence = unidecode.unidecode(sentence)\n",
    "    return sentence\n",
    "\n",
    "def removeSpec(sentence):\n",
    "    sentence = re.sub(\"[^A-Z]\", \" \", sentence, 0, re.IGNORECASE)\n",
    "    return sentence\n",
    "\n",
    "def removeNum(sentence):\n",
    "    pattern = r'[0-9]'\n",
    "    sentence = re.sub(pattern, '', sentence)\n",
    "    return sentence\n",
    "# def preprocessing(data):\n",
    "#     sent_or_word = 'translation_sentence_level'\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: lowerCase(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removePuncu(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeSpec(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeHtml(x))\n",
    "\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeAccented(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeLink(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeNum(x))\n",
    "\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: expand_contractions(x))\n",
    "# #     da[sent_or_word] = da[sent_or_word].apply(lambda x: convertEmoji(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeWhiteSpaces(x))\n",
    "#     x = data[sent_or_word].to_list()\n",
    "#     y = list(data['categorical_labels']) \n",
    "    \n",
    "#     return x,y\n",
    "\n",
    "def lowerCase(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "def removeUser(sentence):\n",
    "    remove = re.compile('(\\s*)user(\\s*)')\n",
    "    sentence = remove.sub(\" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "# with open(r'C:\\Users\\LEGION\\Documents\\Untitled Folder 2\\Emoji_Dict.p', 'rb') as fp:\n",
    "#     Emoji_Dict = pickle.load(fp)\n",
    "# Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "# def convertEmoji(sentence):\n",
    "#     for emot in Emoji_Dict:\n",
    "#         sentence = re.sub(r'('+emot+')', \"\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()),sentence)\n",
    "#     return sentence\n",
    "\n",
    "def add_labels(value):\n",
    "    if value == 'positive':\n",
    "        return 2\n",
    "    elif value == 'negative':\n",
    "        return 0\n",
    "    elif value == 'neutral':\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing(data):\n",
    "#     sent_or_word = 'translation_sentence_level'\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: lowerCase(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removePuncu(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeSpec(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeHtml(x))\n",
    "\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeAccented(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeLink(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeNum(x))\n",
    "\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: expand_contractions(x))\n",
    "# #     da[sent_or_word] = da[sent_or_word].apply(lambda x: convertEmoji(x))\n",
    "#     data[sent_or_word] = data[sent_or_word].apply(lambda x: removeWhiteSpaces(x))\n",
    "#     x = data[sent_or_word].to_list()\n",
    "#     y = list(data['categorical_labels']) \n",
    "    \n",
    "#     return x,y\n",
    "\n",
    "def processing(sent):\n",
    "    removeHtml(sent)\n",
    "    removePuncu(sent)\n",
    "    removeLink(sent)\n",
    "    removeWhiteSpaces(sent)\n",
    "    expand_contractions(sent)\n",
    "    removeAccented(sent)\n",
    "    removeSpec(sent)\n",
    "    removeNum(sent)\n",
    "    lowerCase(sent)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/home/ablstation2/Documents/gaurav/emojiRemovedFiles/test/ig_translated.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>translation_word_level</th>\n",
       "      <th>translation_sentence_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ig_dev_01516</td>\n",
       "      <td>I read kitikpa gba gbuo gi na empathy in my fa...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i read tick run kill you and empathy in my fat...</td>\n",
       "      <td>i read kitikpa go kill you with empathy in my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ig_dev_01400</td>\n",
       "      <td>user user Ọ di egwu 😂</td>\n",
       "      <td>negative</td>\n",
       "      <td>user user He husband fear 😂</td>\n",
       "      <td>user user it's awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ig_dev_00590</td>\n",
       "      <td>user Kedu side ebe o di nwoke oma🙄 Ike umu asa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>user What side somewhere o husband man ok🙄 Str...</td>\n",
       "      <td>user which side is he a good man the strength ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ig_dev_00641</td>\n",
       "      <td>Welcome to KULELEZONE with user amp user NowPl...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Welcome to KULELEZONE with user amp user NowPl...</td>\n",
       "      <td>welcome to kulelezone with user amp user nowpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ig_dev_01812</td>\n",
       "      <td>Azigbakwa ihe ojoo</td>\n",
       "      <td>negative</td>\n",
       "      <td>Also sent something bad</td>\n",
       "      <td>don't send bad things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>1836</td>\n",
       "      <td>ig_dev_00906</td>\n",
       "      <td>user Let it be sir Anwụ cha gị eh ịga ekwete n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>user let it be sir Death casino you eh to go a...</td>\n",
       "      <td>user let it be sir, don't you want to go to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>1837</td>\n",
       "      <td>ig_dev_01792</td>\n",
       "      <td>user Tohwagi onu Ewu</td>\n",
       "      <td>negative</td>\n",
       "      <td>user Tohwagi mouth Goat</td>\n",
       "      <td>user tohwagi's mouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>1838</td>\n",
       "      <td>ig_dev_01097</td>\n",
       "      <td>user Nna adiro m ezo onu agwa mmadu eziokwu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>user Father adiro i hidden mouth Character peo...</td>\n",
       "      <td>user my father hides his mouth and tells peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>1839</td>\n",
       "      <td>ig_dev_00236</td>\n",
       "      <td>user Eee nwanyioma arahurum nke oma Udo🙏</td>\n",
       "      <td>positive</td>\n",
       "      <td>user Yes woman I slept which good Peace 🙏</td>\n",
       "      <td>user yes, my dear, i have been blessed. peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>1840</td>\n",
       "      <td>ig_dev_01062</td>\n",
       "      <td>I ma ofu ihe 2018 guzirim ofuma Ihe ima nigagh...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i but fixed something 2018 I read exit Somethi...</td>\n",
       "      <td>don't pretend that you can. hard man, hard man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1841 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0            ID  \\\n",
       "0              0  ig_dev_01516   \n",
       "1              1  ig_dev_01400   \n",
       "2              2  ig_dev_00590   \n",
       "3              3  ig_dev_00641   \n",
       "4              4  ig_dev_01812   \n",
       "...          ...           ...   \n",
       "1836        1836  ig_dev_00906   \n",
       "1837        1837  ig_dev_01792   \n",
       "1838        1838  ig_dev_01097   \n",
       "1839        1839  ig_dev_00236   \n",
       "1840        1840  ig_dev_01062   \n",
       "\n",
       "                                                  tweet     label  \\\n",
       "0     I read kitikpa gba gbuo gi na empathy in my fa...  negative   \n",
       "1                                 user user Ọ di egwu 😂  negative   \n",
       "2     user Kedu side ebe o di nwoke oma🙄 Ike umu asa...   neutral   \n",
       "3     Welcome to KULELEZONE with user amp user NowPl...   neutral   \n",
       "4                                   Azigbakwa ihe ojoo   negative   \n",
       "...                                                 ...       ...   \n",
       "1836  user Let it be sir Anwụ cha gị eh ịga ekwete n...   neutral   \n",
       "1837                               user Tohwagi onu Ewu  negative   \n",
       "1838        user Nna adiro m ezo onu agwa mmadu eziokwu   neutral   \n",
       "1839           user Eee nwanyioma arahurum nke oma Udo🙏  positive   \n",
       "1840  I ma ofu ihe 2018 guzirim ofuma Ihe ima nigagh...   neutral   \n",
       "\n",
       "                                 translation_word_level  \\\n",
       "0     i read tick run kill you and empathy in my fat...   \n",
       "1                           user user He husband fear 😂   \n",
       "2     user What side somewhere o husband man ok🙄 Str...   \n",
       "3     Welcome to KULELEZONE with user amp user NowPl...   \n",
       "4                               Also sent something bad   \n",
       "...                                                 ...   \n",
       "1836  user let it be sir Death casino you eh to go a...   \n",
       "1837                            user Tohwagi mouth Goat   \n",
       "1838  user Father adiro i hidden mouth Character peo...   \n",
       "1839          user Yes woman I slept which good Peace 🙏   \n",
       "1840  i but fixed something 2018 I read exit Somethi...   \n",
       "\n",
       "                             translation_sentence_level  \n",
       "0     i read kitikpa go kill you with empathy in my ...  \n",
       "1                                user user it's awesome  \n",
       "2     user which side is he a good man the strength ...  \n",
       "3     welcome to kulelezone with user amp user nowpl...  \n",
       "4                                 don't send bad things  \n",
       "...                                                 ...  \n",
       "1836  user let it be sir, don't you want to go to th...  \n",
       "1837                               user tohwagi's mouth  \n",
       "1838  user my father hides his mouth and tells peopl...  \n",
       "1839      user yes, my dear, i have been blessed. peace  \n",
       "1840  don't pretend that you can. hard man, hard man...  \n",
       "\n",
       "[1841 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "1836    False\n",
       "1837    False\n",
       "1838    False\n",
       "1839    False\n",
       "1840    False\n",
       "Name: translation_sentence_level, Length: 1841, dtype: bool"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['translation_sentence_level'].isnull() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 203\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 20%|██        | 1/5 [00:00<00:01,  3.54it/s]***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 256\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                             \n",
      " 20%|██        | 1/5 [00:00<00:01,  3.54it/s]Saving model checkpoint to /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-1\n",
      "Configuration saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5276384353637695, 'eval_accuracy': 0.4074074074074074, 'eval_precision': 0.6088369070825211, 'eval_recall': 0.4074074074074074, 'eval_f1': 0.2955624622291289, 'eval_runtime': 0.0587, 'eval_samples_per_second': 1380.984, 'eval_steps_per_second': 17.049, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-1/pytorch_model.bin\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 40%|████      | 2/5 [00:02<00:03,  1.27s/it]***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 256\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                             \n",
      " 40%|████      | 2/5 [00:02<00:03,  1.27s/it]Saving model checkpoint to /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-2\n",
      "Configuration saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0464285612106323, 'eval_accuracy': 0.5308641975308642, 'eval_precision': 0.47826363998407, 'eval_recall': 0.5308641975308642, 'eval_f1': 0.48388730610952835, 'eval_runtime': 0.0538, 'eval_samples_per_second': 1505.987, 'eval_steps_per_second': 18.592, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-2/pytorch_model.bin\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.62s/it]***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 256\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                             \n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.62s/it]Saving model checkpoint to /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-3\n",
      "Configuration saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2039300203323364, 'eval_accuracy': 0.5432098765432098, 'eval_precision': 0.5435435435435435, 'eval_recall': 0.5432098765432098, 'eval_f1': 0.4423467715503113, 'eval_runtime': 0.0669, 'eval_samples_per_second': 1210.689, 'eval_steps_per_second': 14.947, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-3/pytorch_model.bin\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 80%|████████  | 4/5 [00:06<00:01,  1.78s/it]***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 256\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                             \n",
      " 80%|████████  | 4/5 [00:06<00:01,  1.78s/it]Saving model checkpoint to /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-4\n",
      "Configuration saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.276058554649353, 'eval_accuracy': 0.5308641975308642, 'eval_precision': 0.5320164609053498, 'eval_recall': 0.5308641975308642, 'eval_f1': 0.42265795206971685, 'eval_runtime': 0.0715, 'eval_samples_per_second': 1132.738, 'eval_steps_per_second': 13.984, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-4/pytorch_model.bin\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.86s/it]***** Running Evaluation *****\n",
      "  Num examples = 81\n",
      "  Batch size = 256\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                             \n",
      "100%|██████████| 5/5 [00:08<00:00,  1.86s/it]Saving model checkpoint to /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-5\n",
      "Configuration saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-5/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1989210844039917, 'eval_accuracy': 0.5679012345679012, 'eval_precision': 0.5342375239088855, 'eval_recall': 0.5679012345679012, 'eval_f1': 0.4782089904896922, 'eval_runtime': 0.078, 'eval_samples_per_second': 1038.058, 'eval_steps_per_second': 12.816, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-5/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/ablstation2/Documents/gaurav/translated_dataset/checkpoints/roberta-base_ts_translated/checkpoint-2 (score: 1.0464285612106323).\n",
      "100%|██████████| 5/5 [00:10<00:00,  2.07s/it]\n",
      "Saving model checkpoint to /home/ablstation2/Documents/gaurav/translated_dataset/model/roberta-base_ts_translated\n",
      "Configuration saved in /home/ablstation2/Documents/gaurav/translated_dataset/model/roberta-base_ts_translated/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10.398, 'train_samples_per_second': 97.615, 'train_steps_per_second': 0.481, 'train_loss': 0.824158763885498, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /home/ablstation2/Documents/gaurav/translated_dataset/model/roberta-base_ts_translated/pytorch_model.bin\n",
      "***** Running Prediction *****\n",
      "  Num examples = 203\n",
      "  Batch size = 256\n",
      "/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base\n",
      "accuracy: 0.7684729064039408\n",
      "recall: 0.7684729064039408\n",
      "precision: 0.8218685631534288\n",
      "f1: 0.7236620872531647\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"/home/ablstation2/Documents/gaurav/translated_dataset/model/{}_{}\".format(model_name,x.split(\".\")[0]))\n",
    "\n",
    "test_data = pd.read_csv(\"/home/ablstation2/Documents/gaurav/translated_dataset/test/ts_translated_test.tsv\",sep='\\t')\n",
    "test_data['categorical_labels'] = test_data['label'].map(add_labels)\n",
    "\n",
    "X_test, y_test = preprocessing(test_data)\n",
    "\n",
    "\n",
    "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n",
    "\n",
    "\n",
    "test_dataset = Dataset(X_test_tokenized, y_test)\n",
    "\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "pred = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "  \n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=pred)\n",
    "recall = recall_score(y_true=y_test, y_pred=pred,average='weighted')\n",
    "precision = precision_score(y_true=y_test, y_pred=pred,average='weighted')\n",
    "f1 = f1_score(y_true=y_test, y_pred=pred,average='weighted')\n",
    "\n",
    "print(model_name)\n",
    "print(\"accuracy: {}\".format(accuracy))\n",
    "print(\"recall: {}\".format(recall))\n",
    "print(\"precision: {}\".format(precision))\n",
    "print(\"f1: {}\".format(f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('3.6.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c5472c625688a4fda46769efa78306e9889cc6db92cebd22ca1efa0057f033e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
