{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iKVIo4LMwa60"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import regex as re\n","from bs4 import BeautifulSoup\n","import contractions\n","import unidecode\n","import re\n","import pickle\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","import torch\n","import string\n","from sklearn.utils import shuffle\n","from transformers import AutoTokenizer, AutoModelForMaskedLM,AutoModelForSequenceClassification\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from transformers import TrainingArguments, Trainer\n","from transformers import BertTokenizer, BertForSequenceClassification,RobertaForSequenceClassification,RobertaTokenizer\n","from transformers import EarlyStoppingCallback\n","# from gensim.parsing.preprocessing import remove_stopwords\n","from transformers import XLMRobertaModel, XLMRobertaTokenizer\n","import os\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9ldonc9wa62"},"outputs":[],"source":["\n","def removeHtml(sentence):\n","    soup = BeautifulSoup(sentence, \"html.parser\")\n","    stripped_text = soup.get_text(separator=\" \")\n","    return stripped_text\n","\n","def removePuncu(sentence):\n","    res = re.sub(r'[^\\w\\s]', '', sentence)\n","    return res\n","\n","def removeLink(sentence):\n","    result = re.sub(r'http\\S+', '', sentence)\n","    return result\n","\n","def removeWhiteSpaces(sentence):\n","    sentence = sentence.strip()\n","    return \" \".join(sentence.split())\n","\n","def expand_contractions(sentence):\n","    sentence = contractions.fix(sentence)\n","    return sentence\n","\n","def removeAccented(sentence):\n","    sentence = unidecode.unidecode(sentence)\n","    return sentence\n","\n","def removeSpec(sentence):\n","    sentence = re.sub(\"[^A-Z]\", \" \", sentence, 0, re.IGNORECASE)\n","    return sentence\n","\n","def removeNum(sentence):\n","    pattern = r'[0-9]'\n","    sentence = re.sub(pattern, '', sentence)\n","    return sentence\n","\n","def lowerCase(sentence):\n","    sentence = sentence.lower()\n","    return sentence\n","\n","\n","# with open(r'C:\\Users\\LEGION\\Documents\\Untitled Folder 2\\Emoji_Dict.p', 'rb') as fp:\n","#     Emoji_Dict = pickle.load(fp)\n","# Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n","\n","# def convertEmoji(sentence):\n","#     for emot in Emoji_Dict:\n","#         sentence = re.sub(r'('+emot+')', \"\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()),sentence)\n","#     return sentence\n","\n","def add_labels(value):\n","    if value == 'positive':\n","        return 2\n","    elif value == 'negative':\n","        return 0\n","    elif value == 'neutral':\n","        return 1\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjodxtKcwa62"},"outputs":[],"source":["def preprocessing(data):\n","    sent_or_word = 'translation_sentence_level'\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: lowerCase(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removePuncu(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeSpec(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeHtml(x))\n","\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeAccented(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeLink(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeNum(x))\n","\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: expand_contractions(x))\n","#     da[sent_or_word] = da[sent_or_word].apply(lambda x: convertEmoji(x))\n","    data[sent_or_word] = data[sent_or_word].apply(lambda x: removeWhiteSpaces(x))\n","    # data[sent_or_word] = data[sent_or_word].apply(lambda x: removeUser(x))\n","    x = data[sent_or_word].to_list()\n","    y = list(data['categorical_labels']) \n","    \n","    return x,y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpDV8G41wa63"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czTfhINMwa63"},"outputs":[],"source":["model_results_dict = {\"Model_Name\":[], \"Accuracy\": [], 'Recall': [], 'Precision': [], \"f1\":[]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoBO6fhHwa63","outputId":"b117342f-5c71-44d7-eaa7-b6ff897f2066"},"outputs":[{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/config.json from cache at /home/ablstation2/.cache/huggingface/transformers/7dd97280b5338fb674b5372829a05a1aaaa76f9f2fa71c36199f2ce1ee1104a0.4c7ca95b4fd82b8bbe94fde253f5f82e5a4eedefe6f86f6fa79efc903d6cfe60\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/vocab.json from cache at /home/ablstation2/.cache/huggingface/transformers/089d0f043cfdb86b4f4d79238552b1dcd5b791d4be7c48f27bd7323bdbb7c599.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/merges.txt from cache at /home/ablstation2/.cache/huggingface/transformers/45449f1b6476a9fe84f9eade7f45745cdea8af6b3735f760d8bb0f4b71adf57f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/special_tokens_map.json from cache at /home/ablstation2/.cache/huggingface/transformers/5d7665586d1ae04ace347574fee8f19ad7875acf296e81464f2fb0bb70c0c404.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n","loading file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/config.json from cache at /home/ablstation2/.cache/huggingface/transformers/7dd97280b5338fb674b5372829a05a1aaaa76f9f2fa71c36199f2ce1ee1104a0.4c7ca95b4fd82b8bbe94fde253f5f82e5a4eedefe6f86f6fa79efc903d6cfe60\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/config.json from cache at /home/ablstation2/.cache/huggingface/transformers/7dd97280b5338fb674b5372829a05a1aaaa76f9f2fa71c36199f2ce1ee1104a0.4c7ca95b4fd82b8bbe94fde253f5f82e5a4eedefe6f86f6fa79efc903d6cfe60\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file modelMergedRemoveEmojiTwitterRoberta/twitter-roberta-base-sentiment_Train_merged_Twitter_Roberta_Removed_Emoji/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"modelMergedRemoveEmojiTwitterRoberta/twitter-roberta-base-sentiment_Train_merged_Twitter_Roberta_Removed_Emoji\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.18.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file modelMergedRemoveEmojiTwitterRoberta/twitter-roberta-base-sentiment_Train_merged_Twitter_Roberta_Removed_Emoji/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at modelMergedRemoveEmojiTwitterRoberta/twitter-roberta-base-sentiment_Train_merged_Twitter_Roberta_Removed_Emoji.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"]}],"source":["path = \"modelMergedRemoveEmojiTwitterRoberta/twitter-roberta-base-sentiment_Train_merged_Twitter_Roberta_Removed_Emoji\"\n","# modelMergedRemoveUserTwitterRoberta/twitter-roberta-base-sentiment_Train_merged_Twitter_Roberta_Removed_User\n","# modelMergedRemovedUserRoberta/roberta-base_Train_merged_Roberta_Removed_User\n","max_length = 128\n","num_labels = 3\n","task = 'sentiment'\n","# modelName = \"roberta-base\"\n","modelName = f\"cardiffnlp/twitter-roberta-base-{task}\"\n","\n","# safe_model_name = sanitize(model_name)\n","\n","tokenizer = AutoTokenizer.from_pretrained(modelName)\n","\n","model = AutoModelForSequenceClassification.from_pretrained(path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wK-qKSHWwa64"},"outputs":[],"source":["\n","test_data = pd.read_csv(\"emojiRemovedFiles/MergedEmojiRemoved/testDataMerged.tsv\", sep = '\\t')\n","test_data = test_data[test_data['translation_sentence_level'].notna()]\n","test_data['categorical_labels'] = test_data['label'].map(add_labels)\n","X_test, y_test = preprocessing(test_data)\n","\n","X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=max_length,add_special_tokens=True)\n","test_dataset = Dataset(X_test_tokenized)"]},{"cell_type":"code","source":[],"metadata":{"id":"TmpqHwnnjNxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdZ4b9_ywa64"},"outputs":[],"source":["def compute_metrics(p):\n","        pred, labels = p\n","        pred = np.argmax(pred, axis=1)\n","        accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","        recall = recall_score(y_true=labels, y_pred=pred,average='weighted')\n","        precision = precision_score(y_true=labels, y_pred=pred,average='weighted')\n","        f1 = f1_score(y_true=labels, y_pred=pred,average='weighted')\n","\n","        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWeDUxdRwa65"},"outputs":[],"source":["newModelNameForSaving = modelName.split(\"/\")[-1]\n","x = \"Test_mergedTwitterRobertaRemovedEmoji.tsv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlLC8wUGwa65","outputId":"354b0747-cb96-4093-d5c7-556bdb61adbf"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","100%|██████████| 23/23 [01:53<00:00,  4.95s/it]\n"]}],"source":["args = TrainingArguments(\n","    output_dir=\"checkpointsTestMerRemUser/{}_{}\".format(newModelNameForSaving,x.split(\".\")[0]),\n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=256,\n","    per_device_eval_batch_size=256,\n","    num_train_epochs=5,\n","    seed=7,\n","    load_best_model_at_end=True,\n","    save_strategy='epoch',\n","    overwrite_output_dir=True\n","      #save_strategy = \"no\"\n","  )\n","\n","trainer = Trainer(\n","      model=model,\n","      args=args,\n","      compute_metrics=compute_metrics,\n","      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n","  )\n"]},{"cell_type":"code","source":[],"metadata":{"id":"32uxk2NYjrVI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZMjARN5wa65"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIHjiS5Jwa66","outputId":"2d7ef48d-3fa1-4a60-ed5f-efd18242d399"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 11647\n","  Batch size = 256\n","/home/ablstation2/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","100%|██████████| 23/23 [00:05<00:00,  4.16it/s]"]},{"name":"stdout","output_type":"stream","text":["cardiffnlp/twitter-roberta-base-sentiment\n","accuracy: 0.66429123379411\n","recall: 0.66429123379411\n","precision: 0.6731019432438173\n","f1: 0.661372015086978\n"]}],"source":["predictions = trainer.predict(test_dataset)\n","\n","pred = np.argmax(predictions.predictions, axis=-1)\n","\n","  \n","accuracy = accuracy_score(y_true=y_test, y_pred=pred)\n","recall = recall_score(y_true=y_test, y_pred=pred,average='weighted')\n","precision = precision_score(y_true=y_test, y_pred=pred,average='weighted')\n","f1 = f1_score(y_true=y_test, y_pred=pred,average='weighted')\n","\n","print(modelName)\n","print(\"accuracy: {}\".format(accuracy))\n","print(\"recall: {}\".format(recall))\n","print(\"precision: {}\".format(precision))\n","print(\"f1: {}\".format(f1))\n","\n","\n","model_results_dict['Model_Name'].append(x)\n","model_results_dict['Accuracy'].append(accuracy)\n","model_results_dict['Recall'].append(recall)\n","model_results_dict['Precision'].append(precision)\n","model_results_dict['f1'].append(f1)\n","\n","df = pd.DataFrame(data = model_results_dict)\n","df.to_csv(\"{}_MergedDataTwitterRobertaRemovedEmojiTestResults.csv\".format(newModelNameForSaving))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0WtfIXPwa66"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.6.5 64-bit ('3.6.5')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"9c5472c625688a4fda46769efa78306e9889cc6db92cebd22ca1efa0057f033e"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}